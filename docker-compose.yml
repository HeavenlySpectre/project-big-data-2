services:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ZOOKEEPER & KAFKA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports: ["2181:2181"]
    restart: always
    networks: [bigdata_network]

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092" # host access
      - "9093:9093" # inter-container
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "game.reviews:1:1"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on: [zookeeper]
    restart: always
    networks: [bigdata_network]

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ KAFKA PRODUCER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  producer:
    image: python:3.9-slim
    container_name: producer
    working_dir: /app
    volumes:
      - ./producer_app:/app
      - ./data:/datasets_to_read:ro
      - ./requirements_producer.txt:/app/requirements.txt
    command: >
      sh -c "pip install --no-cache-dir -r requirements.txt &&
             python /app/producer.py
             --file /datasets_to_read/realrecommendations.csv
             --topic game.reviews
             --bootstrap_servers kafka:9093"
    depends_on: [kafka]
    restart: on-failure
    networks: [bigdata_network]

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ KAFKA CONSUMER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  consumer:
    image: python:3.9-slim
    container_name: consumer
    working_dir: /app
    volumes:
      - ./consumer_app:/app
      - pipeline_data:/data
      - ./requirements_consumer.txt:/app/requirements.txt
    command: >
      sh -c "pip install --no-cache-dir -r requirements.txt &&
             python /app/consumer.py
             --topic game.reviews
             --bootstrap_servers kafka:9093
             --output_path /data"
    depends_on: [kafka]
    restart: on-failure
    networks: [bigdata_network]

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HADOOP CLUSTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports: ["9870:9870"] # Web UI
    volumes: [hadoop_namenode:/hadoop/dfs/name]
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000 # Updated port
      - HDFS_CONF_dfs_namenode_rpc-address=namenode:9000 # Updated port
    restart: always
    networks: [bigdata_network]

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    volumes: [hadoop_datanode:/hadoop/dfs/data]
    environment:
      SERVICE_PRECONDITION: "namenode:9870" # Checks NameNode WebUI, 9000 for RPC will be via fs.defaultFS
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000 # Updated port
      HDFS_CONF_dfs_namenode_rpc-address: namenode:9000 # Updated port
    depends_on: [namenode]
    restart: always
    networks: [bigdata_network]

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    ports: ["8088:8088"]
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864" # Updated port for NameNode RPC
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000 # Updated port
      HDFS_CONF_dfs_namenode_rpc-address: namenode:9000 # Updated port
    depends_on: [namenode, datanode]
    restart: always
    networks: [bigdata_network]

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager1
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088" # Updated port for NameNode RPC
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000 # Updated port
      HDFS_CONF_dfs_namenode_rpc-address: namenode:9000 # Updated port
    depends_on: [namenode, datanode, resourcemanager]
    restart: always
    networks: [bigdata_network]

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    ports: ["19888:19888"]
    volumes: [hadoop_historyserver:/hadoop/yarn/timeline]
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088" # Updated port for NameNode RPC
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000 # Updated port
      HDFS_CONF_dfs_namenode_rpc-address: namenode:9000 # Updated port
    depends_on: [namenode, datanode, resourcemanager]
    restart: always
    networks: [bigdata_network]
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SPARK CLUSTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8081:8080" # Web UI
      - "7077:7077" # RPC
    volumes:
      - ./spark_app:/app
      - pipeline_data:/input_parquet_data
      - ./data:/input_datasets:ro # Mount CSV datasets
      - model_storage:/models_output
      - ./hadoop_conf:/etc/hadoop/conf:ro # Mount the Hadoop configuration
    environment:
      - SPARK_NO_DAEMONIZE=true
      - HADOOP_CONF_DIR=/etc/hadoop/conf # This will now point to the mounted config
      - YARN_CONF_DIR=/etc/hadoop/conf
    depends_on: [kafka, namenode, resourcemanager, consumer]
    restart: always
    networks: [bigdata_network]

  spark-trainer:
    image: python:3.9-slim
    container_name: spark-trainer
    volumes:
      - ./spark_app:/app
      - pipeline_data:/input_parquet_data
      - ./data:/input_datasets:ro # Mount CSV datasets
      - model_storage:/models_output
      - ./hadoop_conf:/etc/hadoop/conf:ro # Mount the Hadoop configuration
    command: >
      sh -c "echo 'ðŸ”§ Installing dependencies...' &&
             apt-get update -qq &&
             apt-get install -y --no-install-recommends openjdk-17-jdk-headless wget &&
             echo 'ðŸ“¦ Installing Python packages...' &&
             pip install --no-cache-dir numpy pyspark==3.3.0 &&
             echo 'â¬‡ï¸ Downloading Spark...' &&
             wget -q https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz &&
             tar -xzf spark-3.3.0-bin-hadoop3.tgz &&
             mv spark-3.3.0-bin-hadoop3 /opt/spark &&
             rm spark-3.3.0-bin-hadoop3.tgz &&
             export SPARK_HOME=/opt/spark &&
             export PATH=/opt/spark/bin:/opt/spark/sbin:$$PATH &&
             echo 'ðŸš€ Starting Spark training job...' &&
             /opt/spark/bin/spark-submit --master spark://spark-master:7077 --deploy-mode client --conf spark.sql.files.ignoreCorruptFiles=true --conf spark.sql.files.ignoreMissingFiles=true --conf spark.hadoop.fs.file.impl=org.apache.hadoop.fs.LocalFileSystem --conf spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs=false /app/spark_job.py --input file:///input_parquet_data --output file:///models_output &&
             echo 'âœ… Training completed!'"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/etc/hadoop/conf
      - YARN_CONF_DIR=/etc/hadoop/conf
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    depends_on:
      - spark-master
      - spark-worker-1
      - consumer
    restart: "no"
    networks: [bigdata_network]
  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - HADOOP_CONF_DIR=/etc/hadoop/conf # This will now point to the mounted config
      - YARN_CONF_DIR=/etc/hadoop/conf
    volumes:
      - ./spark_app:/app
      - pipeline_data:/input_parquet_data
      - ./data:/input_datasets:ro # Mount CSV datasets
      - model_storage:/models_output
      - ./hadoop_conf:/etc/hadoop/conf:ro # Mount the Hadoop configuration
    depends_on: [spark-master]
    restart: always
    networks: [bigdata_network]

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FASTAPI SERVICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  api:
    image: python:3.9-slim
    container_name: api
    working_dir: /app
    ports: ["8000:8000"]
    volumes:
      - ./api_app:/app
      - model_storage:/models_input:ro
      - ./requirements_api.txt:/app/requirements.txt
    command: >
      sh -c "apt-get update -qq &&
           apt-get install -y --no-install-recommends default-jre-headless &&
           pip install --no-cache-dir numpy pyspark -r requirements.txt &&
           uvicorn main:app --host 0.0.0.0 --port 8000 --reload"
    environment:
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    depends_on: [spark-master]
    restart: always
    networks: [bigdata_network]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VOLUMES & NETWORK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  pipeline_data:
  model_storage:
  raw_datasets:

networks:
  bigdata_network:
    driver: bridge
    name: bigdata_network
